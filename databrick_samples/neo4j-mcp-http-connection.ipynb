{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j MCP Server HTTP Connection Setup (AWS AgentCore)\n",
    "\n",
    "This notebook demonstrates how to create a Databricks HTTP connection to the Neo4j MCP server deployed on AWS AgentCore Gateway. Once configured, you can query Neo4j graph data directly from SQL using the `http_request` function.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Validates that OAuth2 secrets are configured in Databricks\n",
    "2. Creates an HTTP connection in Unity Catalog with OAuth2 M2M authentication\n",
    "3. Tests the connection by calling MCP tools (get-schema, read-cypher)\n",
    "4. Demonstrates how to parse and use the results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Databricks Runtime**: 15.4 LTS or later, or SQL warehouse 2023.40+\n",
    "- **Unity Catalog**: Must be enabled on your workspace\n",
    "- **AWS AgentCore deployed**: The Neo4j MCP server must be deployed via `neo4j-agentcore-mcp-server`\n",
    "- **Secrets configured**: Run `setup_databricks_secrets.sh` before this notebook\n",
    "\n",
    "## Authentication\n",
    "\n",
    "This integration uses **OAuth2 Machine-to-Machine (M2M)** authentication via AWS Cognito. Databricks handles token exchange and refresh automatically - no manual token management is required.\n",
    "\n",
    "## Security Note\n",
    "\n",
    "This integration provides **READ-ONLY** access to Neo4j. The `write-cypher` tool is intentionally excluded to prevent accidental data modifications from analytics workflows."
   ],
   "id": "markdown-3d3e6b1d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these values to match your environment. The secret scope should match what you used when running `setup_databricks_secrets.sh`."
   ],
   "id": "markdown-b4853132"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - update these values for your environment\n",
    "SECRET_SCOPE = \"mcp-neo4j-secrets\"           # Must match the scope used in setup_databricks_secrets.sh\n",
    "CONNECTION_NAME = \"neo4j_agentcore_mcp\"      # Name for the HTTP connection (metastore-level, not catalog-scoped)\n",
    "\n",
    "# Tool names are prefixed by AgentCore Gateway with the target name\n",
    "TOOL_GET_SCHEMA = \"neo4j-mcp-server-target___get-schema\"\n",
    "TOOL_READ_CYPHER = \"neo4j-mcp-server-target___read-cypher\""
   ],
   "id": "code-aa831383"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Validate Secrets\n",
    "\n",
    "First, verify that the required OAuth2 secrets exist in Databricks. If this step fails, run `setup_databricks_secrets.sh` from your local machine."
   ],
   "id": "markdown-8597fb38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that OAuth2 secrets are configured\n",
    "try:\n",
    "    gateway_host = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"gateway_host\")\n",
    "    client_id = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"client_id\")\n",
    "    client_secret = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"client_secret\")\n",
    "    token_endpoint = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"token_endpoint\")\n",
    "    oauth_scope = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"oauth_scope\")\n",
    "    \n",
    "    print(f\"OAuth2 secrets validated successfully!\")\n",
    "    print(f\"  Gateway Host: {gateway_host}\")\n",
    "    print(f\"  Client ID: {client_id}\")\n",
    "    print(f\"  Client Secret: [REDACTED - {len(client_secret)} characters]\")\n",
    "    print(f\"  Token Endpoint: {token_endpoint}\")\n",
    "    print(f\"  OAuth Scope: {oauth_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to retrieve secrets from scope '{SECRET_SCOPE}'\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\")\n",
    "    print(\"To fix this, run the setup script from your local machine:\")\n",
    "    print(f\"  cd databrick_samples && ./setup_databricks_secrets.sh {SECRET_SCOPE}\")\n",
    "    print(\"\")\n",
    "    print(\"Make sure the AgentCore MCP server is deployed first:\")\n",
    "    print(\"  cd neo4j-agentcore-mcp-server && ./deploy.sh && ./deploy.sh credentials\")\n",
    "    raise"
   ],
   "id": "code-4124111c"
  },
  {
   "cell_type": "markdown",
   "id": "d9y9fkf3bkv",
   "source": "## Step 2: Preflight Connection Checks\n\nBefore creating the HTTP connection, verify that Databricks can actually reach the AgentCore Gateway and exchange OAuth2 tokens. These checks catch the most common issues:\n\n1. **Network connectivity** — Can Databricks reach the Gateway endpoint? (VPC egress, firewalls)\n2. **OAuth2 token exchange** — Can Databricks get a JWT from Cognito? (credentials, scopes)\n3. **Existing connection** — Does the connection need to be dropped and recreated after a secrets update?\n\nIf `./cloud.sh` works locally but Databricks can't connect, the issue is almost always network egress or stale connections.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "c5zk7yw40m6",
   "source": "import requests\nimport json\nfrom urllib.parse import urlparse\n\nprint(\"=\" * 60)\nprint(\"PREFLIGHT CHECK 1: Network Connectivity to Gateway\")\nprint(\"=\" * 60)\nprint()\nprint(f\"Gateway Host: {gateway_host}\")\n\n# Test basic HTTPS connectivity to the Gateway endpoint\n# We expect a 401/403 (auth required) — that means the network path works.\n# A timeout or connection error means Databricks can't reach the Gateway.\ntry:\n    resp = requests.get(f\"{gateway_host}/mcp\", timeout=10)\n    if resp.status_code in (401, 403):\n        print(f\"  Status: {resp.status_code} (auth required — expected)\")\n        print(\"  PASSED — Gateway is reachable from Databricks\")\n    elif resp.status_code == 405:\n        print(f\"  Status: {resp.status_code} (method not allowed — expected for GET on MCP endpoint)\")\n        print(\"  PASSED — Gateway is reachable from Databricks\")\n    elif 200 <= resp.status_code < 500:\n        print(f\"  Status: {resp.status_code}\")\n        print(\"  PASSED — Gateway is reachable from Databricks\")\n    else:\n        print(f\"  Status: {resp.status_code}\")\n        print(f\"  Response: {resp.text[:200]}\")\n        print(\"  WARNING — Unexpected status, but Gateway is reachable\")\nexcept requests.exceptions.ConnectionError as e:\n    print(f\"  FAILED — Cannot connect to Gateway\")\n    print(f\"  Error: {e}\")\n    print()\n    print(\"  This means Databricks cannot reach the AgentCore Gateway.\")\n    print(\"  Common causes:\")\n    print(\"    - Workspace VPC has restricted egress (firewall/security groups)\")\n    print(\"    - Gateway endpoint is not publicly accessible\")\n    print(\"    - DNS resolution failure\")\n    raise RuntimeError(\"Gateway unreachable from Databricks — check VPC egress rules\")\nexcept requests.exceptions.Timeout:\n    print(\"  FAILED — Connection timed out (10s)\")\n    print()\n    print(\"  The Gateway endpoint is not responding. Common causes:\")\n    print(\"    - VPC egress is blocked for this destination\")\n    print(\"    - AgentCore Gateway is not running\")\n    print(\"  Verify from your local machine: cd neo4j-agentcore-mcp-server && ./cloud.sh\")\n    raise RuntimeError(\"Gateway connection timeout — check VPC egress and Gateway status\")\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f9v5tf3wt69",
   "source": "print(\"=\" * 60)\nprint(\"PREFLIGHT CHECK 2: OAuth2 Token Exchange (Cognito)\")\nprint(\"=\" * 60)\nprint()\nprint(f\"Token Endpoint: {token_endpoint}\")\nprint(f\"Client ID: {client_id}\")\nprint(f\"OAuth Scope: {oauth_scope}\")\nprint()\n\n# Test the client_credentials OAuth2 flow — this is exactly what Databricks\n# does behind the scenes when using the HTTP connection.\ntry:\n    token_resp = requests.post(\n        token_endpoint,\n        data={\n            \"grant_type\": \"client_credentials\",\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n            \"scope\": oauth_scope,\n        },\n        headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        timeout=10,\n    )\n\n    if token_resp.status_code == 200:\n        token_data = token_resp.json()\n        access_token = token_data.get(\"access_token\", \"\")\n        expires_in = token_data.get(\"expires_in\", \"unknown\")\n        token_type = token_data.get(\"token_type\", \"unknown\")\n        print(f\"  Token Type: {token_type}\")\n        print(f\"  Expires In: {expires_in}s\")\n        print(f\"  Token: {access_token[:40]}...\")\n        print(\"  PASSED — OAuth2 client_credentials flow works\")\n    else:\n        print(f\"  Status: {token_resp.status_code}\")\n        print(f\"  Response: {token_resp.text[:300]}\")\n        print(\"  FAILED — Could not obtain OAuth2 token\")\n        print()\n        print(\"  Common causes:\")\n        print(\"    - Invalid client_id or client_secret (re-run: ./deploy.sh credentials)\")\n        print(\"    - Cognito token endpoint unreachable from Databricks\")\n        print(\"    - OAuth scope mismatch\")\n        raise RuntimeError(f\"OAuth2 token exchange failed: HTTP {token_resp.status_code}\")\n\nexcept requests.exceptions.ConnectionError as e:\n    print(f\"  FAILED — Cannot connect to Cognito\")\n    print(f\"  Error: {e}\")\n    print()\n    print(\"  Databricks cannot reach the Cognito token endpoint.\")\n    print(\"  This is a separate network dependency from the Gateway.\")\n    parsed = urlparse(token_endpoint)\n    print(f\"  Ensure egress is allowed to: {parsed.hostname}\")\n    raise RuntimeError(\"Cognito token endpoint unreachable — check VPC egress rules\")\nexcept requests.exceptions.Timeout:\n    print(\"  FAILED — Token endpoint timed out (10s)\")\n    raise RuntimeError(\"Cognito token endpoint timeout — check VPC egress rules\")\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jsafvs5vn2",
   "source": "print(\"=\" * 60)\nprint(\"PREFLIGHT CHECK 3: Authenticated Gateway Request\")\nprint(\"=\" * 60)\nprint()\n\n# Use the token we just obtained to make an actual MCP request to the Gateway.\n# This mirrors what cloud.sh does — send a tools/list JSON-RPC request.\ntry:\n    mcp_request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"tools/list\",\n        \"id\": 1,\n    }\n    mcp_resp = requests.post(\n        f\"{gateway_host}/mcp\",\n        json=mcp_request,\n        headers={\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\",\n        },\n        timeout=30,\n    )\n\n    if mcp_resp.status_code == 200:\n        # Response could be JSON or SSE (text/event-stream)\n        content_type = mcp_resp.headers.get(\"Content-Type\", \"\")\n        if \"event-stream\" in content_type:\n            # Parse SSE — extract the last JSON data line\n            lines = mcp_resp.text.strip().split(\"\\n\")\n            data_lines = [l.removeprefix(\"data: \") for l in lines if l.startswith(\"data: \")]\n            if data_lines:\n                mcp_data = json.loads(data_lines[-1])\n            else:\n                mcp_data = {\"raw\": mcp_resp.text[:300]}\n        else:\n            mcp_data = mcp_resp.json()\n\n        tools = mcp_data.get(\"result\", {}).get(\"tools\", [])\n        if tools:\n            print(f\"  Found {len(tools)} MCP tools:\")\n            for t in tools:\n                desc = (t.get(\"description\") or \"\")[:60]\n                print(f\"    - {t['name']}: {desc}...\")\n            print(\"  PASSED — Gateway returns MCP tools\")\n        else:\n            print(f\"  Response: {json.dumps(mcp_data)[:300]}\")\n            print(\"  WARNING — Got 200 but no tools in response (check MCP server status)\")\n    else:\n        print(f\"  Status: {mcp_resp.status_code}\")\n        print(f\"  Response: {mcp_resp.text[:300]}\")\n        print(\"  FAILED — Authenticated MCP request failed\")\n        print()\n        if mcp_resp.status_code in (401, 403):\n            print(\"  The token was obtained but the Gateway rejected it.\")\n            print(\"  This can happen if the Cognito resource server or scope is misconfigured.\")\n        raise RuntimeError(f\"MCP tools/list failed: HTTP {mcp_resp.status_code}\")\n\nexcept requests.exceptions.Timeout:\n    print(\"  FAILED — MCP request timed out (30s)\")\n    print(\"  The Gateway is reachable but the MCP server may not be responding.\")\n    print(\"  Check: cd neo4j-agentcore-mcp-server && ./deploy.sh status\")\n    raise RuntimeError(\"MCP request timeout — check MCP server status\")\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qp9y3arkr1",
   "source": "print(\"=\" * 60)\nprint(\"PREFLIGHT CHECK 4: Existing Connection Status\")\nprint(\"=\" * 60)\nprint()\n\n# Check if the connection already exists. If secrets were updated, the\n# connection must be dropped and recreated — CREATE CONNECTION IF NOT EXISTS\n# won't pick up new secret values.\ntry:\n    all_connections = spark.sql(\"SHOW CONNECTIONS\").collect()\n    match = [r for r in all_connections if r[\"name\"] == CONNECTION_NAME]\n    if match:\n        print(f\"  Connection '{CONNECTION_NAME}' already exists.\")\n        print()\n        print(\"  WARNING: If you recently updated secrets (re-ran setup_databricks_secrets.sh),\")\n        print(\"  the existing connection is still using the OLD credential values.\")\n        print(\"  Databricks caches connection options at creation time.\")\n        print()\n        print(\"  To fix: uncomment and run the DROP CONNECTION cell below,\")\n        print(\"  then re-run the CREATE CONNECTION cell.\")\n        print()\n        print(\"  Or run this now:\")\n        print(f\"    spark.sql(\\\"DROP CONNECTION IF EXISTS {CONNECTION_NAME}\\\")\")\n    else:\n        print(f\"  No existing connection '{CONNECTION_NAME}' found — will create fresh.\")\n        print(\"  PASSED\")\nexcept Exception as e:\n    # SHOW CONNECTIONS may not be available on all Databricks tiers\n    print(f\"  Could not check existing connections: {e}\")\n    print(\"  Skipping — will attempt CREATE CONNECTION IF NOT EXISTS\")\nprint()\n\nprint(\"=\" * 60)\nprint(\"ALL PREFLIGHT CHECKS COMPLETE\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Create the HTTP Connection\n\nCreate an HTTP connection in Unity Catalog. This connection:\n- Points to the AgentCore Gateway endpoint\n- Uses OAuth2 M2M authentication with Cognito credentials\n- Databricks automatically handles token exchange and refresh\n- Can be shared with other users via Unity Catalog permissions\n\n**Implementation Note**: We use `spark.sql()` with Python f-strings instead of `%%sql` magic because the `CREATE CONNECTION` OPTIONS clause requires constant expressions.\n\n**Scope Note**: Connections are **metastore-level objects** in Unity Catalog, not catalog-scoped. They're shared across all catalogs in your workspace.\n\n**MCP Integration Note**: For full Databricks AI/MCP integration features, you may need to manually enable the \"Is mcp connection\" checkbox in the Catalog Explorer UI after creating the connection. Navigate to: **Catalog > External Data > Connections > neo4j_agentcore_mcp > Edit** and check the MCP option.\n\n**Note**: If the connection already exists, you'll see an error. Use the cleanup cell at the bottom to drop it first, then re-run this cell.",
   "id": "markdown-f8fbde97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display connection parameters (secrets are not exposed)\n",
    "print(f\"Creating HTTP connection '{CONNECTION_NAME}'...\")\n",
    "print(f\"  Gateway Host: {gateway_host}\")\n",
    "print(f\"  Base Path: /mcp\")\n",
    "print(f\"  Authentication: OAuth2 M2M (Databricks handles token refresh)\")\n",
    "print(f\"  Secret Scope: {SECRET_SCOPE}\")"
   ],
   "id": "code-e23b874c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HTTP connection with OAuth2 M2M authentication\n",
    "# Databricks automatically handles token exchange and refresh\n",
    "# Note: Using spark.sql() because OPTIONS requires constant expressions (no variable interpolation in %%sql)\n",
    "# Note: Connections are metastore-level objects, not catalog-scoped\n",
    "\n",
    "create_connection_sql = f\"\"\"\n",
    "CREATE CONNECTION IF NOT EXISTS {CONNECTION_NAME} TYPE HTTP\n",
    "OPTIONS (\n",
    "  host secret('{SECRET_SCOPE}', 'gateway_host'),\n",
    "  base_path '/mcp',\n",
    "  client_id secret('{SECRET_SCOPE}', 'client_id'),\n",
    "  client_secret secret('{SECRET_SCOPE}', 'client_secret'),\n",
    "  oauth_scope secret('{SECRET_SCOPE}', 'oauth_scope'),\n",
    "  token_endpoint secret('{SECRET_SCOPE}', 'token_endpoint')\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing SQL:\")\n",
    "print(create_connection_sql)\n",
    "\n",
    "spark.sql(create_connection_sql)\n",
    "print(f\"\\nConnection created: {CONNECTION_NAME}\")"
   ],
   "id": "code-f70f08bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection was created\n",
    "print(f\"Connection '{CONNECTION_NAME}' created successfully!\")\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"IMPORTANT: Manual Step Required for MCP Integration\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\")\n",
    "print(\"The connection is created but won't appear in the Playground\")\n",
    "print(\"'External MCP Servers' dropdown until you enable the MCP flag:\")\n",
    "print(\"\")\n",
    "print(\"  1. Go to Catalog Explorer > External Data > Connections\")\n",
    "print(f\"  2. Find '{CONNECTION_NAME}' and click Edit\")\n",
    "print(\"  3. Check the 'Is MCP connection' checkbox\")\n",
    "print(\"  4. Save the connection\")\n",
    "print(\"\")\n",
    "print(\"After that, you can use it in the Playground:\")\n",
    "print(\"  Add tools > MCP Servers > External MCP Servers > Unity Catalog Connection\")\n",
    "print(\"\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "code-9d5a7624"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Test the Connection - List Tools\n\nThe MCP protocol uses JSON-RPC. Let's first list the available tools to verify the connection works.\n\n**Note**: Tool names are prefixed by the AgentCore Gateway with the target name:\n- `neo4j-mcp-server-target___get-schema`\n- `neo4j-mcp-server-target___read-cypher`",
   "id": "markdown-fe78693f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# MCP JSON-RPC request to list tools\n",
    "list_tools_request = json.dumps({\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/list\",\n",
    "    \"id\": 1\n",
    "})\n",
    "\n",
    "print(\"Request payload:\")\n",
    "print(list_tools_request)"
   ],
   "id": "code-fbe42813"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available MCP tools\n",
    "list_tools_sql = f\"\"\"\n",
    "SELECT http_request(\n",
    "  conn => '{CONNECTION_NAME}',\n",
    "  method => 'POST',\n",
    "  path => '',\n",
    "  headers => map('Content-Type', 'application/json'),\n",
    "  json => '{{\"jsonrpc\":\"2.0\",\"method\":\"tools/list\",\"id\":1}}'\n",
    ") AS response\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(list_tools_sql)\n",
    "display(result)"
   ],
   "id": "code-c0e3af56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Get Neo4j Schema\n\nCall the `get-schema` tool to retrieve the Neo4j database schema, including node labels, relationship types, and properties.\n\n**Note**: The tool name is prefixed: `neo4j-mcp-server-target___get-schema`",
   "id": "markdown-058ef9c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Neo4j database schema using the Gateway-prefixed tool name\n",
    "get_schema_sql = f\"\"\"\n",
    "SELECT http_request(\n",
    "  conn => '{CONNECTION_NAME}',\n",
    "  method => 'POST',\n",
    "  path => '',\n",
    "  headers => map('Content-Type', 'application/json'),\n",
    "  json => '{{\"jsonrpc\":\"2.0\",\"method\":\"tools/call\",\"params\":{{\"name\":\"{TOOL_GET_SCHEMA}\",\"arguments\":{{}}}},\"id\":2}}'\n",
    ") AS response\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(get_schema_sql)\n",
    "display(result)"
   ],
   "id": "code-7387d32a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Execute a Read Query\n\nCall the `read-cypher` tool to execute a read-only Cypher query against Neo4j.\n\n**Important**: Only read queries are permitted through this connection. The `write-cypher` tool is intentionally not exposed.\n\n**Note**: The tool name is prefixed: `neo4j-mcp-server-target___read-cypher`",
   "id": "markdown-897f14a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example: Count nodes by label\n",
    "cypher_query = \"MATCH (n) RETURN labels(n) AS label, count(*) AS count ORDER BY count DESC LIMIT 10\"\n",
    "\n",
    "# Build the MCP request with Gateway-prefixed tool name\n",
    "read_cypher_request = json.dumps({\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/call\",\n",
    "    \"params\": {\n",
    "        \"name\": TOOL_READ_CYPHER,\n",
    "        \"arguments\": {\n",
    "            \"query\": cypher_query\n",
    "        }\n",
    "    },\n",
    "    \"id\": 3\n",
    "})\n",
    "\n",
    "print(\"Cypher query:\")\n",
    "print(cypher_query)\n",
    "print(\"\")\n",
    "print(\"MCP request:\")\n",
    "print(read_cypher_request)"
   ],
   "id": "code-e813748d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a read-only Cypher query\n",
    "# Escape single quotes for SQL\n",
    "escaped_request = read_cypher_request.replace(\"'\", \"''\")\n",
    "\n",
    "cypher_sql = f\"\"\"\n",
    "SELECT http_request(\n",
    "  conn => '{CONNECTION_NAME}',\n",
    "  method => 'POST',\n",
    "  path => '',\n",
    "  headers => map('Content-Type', 'application/json'),\n",
    "  json => '{escaped_request}'\n",
    ") AS response\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(cypher_sql)\n",
    "display(result)"
   ],
   "id": "code-18c3908b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Parse and Display Results\n\nThe MCP response is JSON. Let's parse it and display the results in a more readable format.",
   "id": "markdown-729efd61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, get_json_object\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
    "\n",
    "# Execute the query and get the response\n",
    "escaped_request = read_cypher_request.replace(\"'\", \"''\")\n",
    "\n",
    "result_df = spark.sql(f\"\"\"\n",
    "    SELECT http_request(\n",
    "      conn => '{CONNECTION_NAME}',\n",
    "      method => 'POST',\n",
    "      path => '',\n",
    "      headers => map('Content-Type', 'application/json'),\n",
    "      json => '{escaped_request}'\n",
    "    ) AS response\n",
    "\"\"\")\n",
    "\n",
    "# Extract the response body\n",
    "response_row = result_df.first()\n",
    "if response_row:\n",
    "    response = response_row[\"response\"]\n",
    "    print(\"Raw response:\")\n",
    "    print(json.dumps(json.loads(response[\"text\"]), indent=2) if \"text\" in response else response)"
   ],
   "id": "code-3e5de755"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Query Neo4j\n",
    "\n",
    "Here's a reusable function to query Neo4j through the MCP connection."
   ],
   "id": "markdown-79f349f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def query_neo4j(cypher_query: str, connection_name: str = None, tool_name: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Execute a read-only Cypher query against Neo4j via the MCP HTTP connection.\n",
    "    \n",
    "    Args:\n",
    "        cypher_query: The Cypher query to execute (read-only)\n",
    "        connection_name: Name of the HTTP connection (default: uses CONNECTION_NAME from notebook)\n",
    "        tool_name: The MCP tool name (default: uses TOOL_READ_CYPHER with Gateway prefix)\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON response from Neo4j\n",
    "    \"\"\"\n",
    "    if connection_name is None:\n",
    "        connection_name = CONNECTION_NAME\n",
    "    if tool_name is None:\n",
    "        tool_name = TOOL_READ_CYPHER\n",
    "    \n",
    "    # Build the MCP request with Gateway-prefixed tool name\n",
    "    request_payload = json.dumps({\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"tools/call\",\n",
    "        \"params\": {\n",
    "            \"name\": tool_name,\n",
    "            \"arguments\": {\n",
    "                \"query\": cypher_query\n",
    "            }\n",
    "        },\n",
    "        \"id\": 1\n",
    "    })\n",
    "    \n",
    "    # Escape single quotes for SQL\n",
    "    escaped_payload = request_payload.replace(\"'\", \"''\")\n",
    "    \n",
    "    # Execute the query\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "        SELECT http_request(\n",
    "          conn => '{connection_name}',\n",
    "          method => 'POST',\n",
    "          path => '',\n",
    "          headers => map('Content-Type', 'application/json'),\n",
    "          json => '{escaped_payload}'\n",
    "        ) AS response\n",
    "    \"\"\")\n",
    "    \n",
    "    # Parse the response\n",
    "    response_row = result_df.first()\n",
    "    if response_row and \"response\" in response_row.asDict():\n",
    "        response = response_row[\"response\"]\n",
    "        if \"text\" in response:\n",
    "            return json.loads(response[\"text\"])\n",
    "    return None\n",
    "\n",
    "print(\"Helper function 'query_neo4j' is now available.\")\n",
    "print(\"\")\n",
    "print(\"Example usage:\")\n",
    "print(f'  result = query_neo4j(\"MATCH (n) RETURN count(n) AS total\")')\n",
    "print(f\"  # Uses connection: {CONNECTION_NAME}\")\n",
    "print(f\"  # Uses tool: {TOOL_READ_CYPHER}\")"
   ],
   "id": "code-cffd229e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use the helper function\n",
    "result = query_neo4j(\"MATCH (n) RETURN count(n) AS total\")\n",
    "print(\"Query result:\")\n",
    "print(json.dumps(result, indent=2))"
   ],
   "id": "code-1697641a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**1. Secret not found**\n",
    "```\n",
    "Secret does not exist with scope: mcp-neo4j-secrets and key: gateway_host\n",
    "```\n",
    "Solution: Run `./setup_databricks_secrets.sh` from the `databrick_samples` directory.\n",
    "\n",
    "**2. Connection already exists**\n",
    "```\n",
    "Connection 'neo4j_agentcore_mcp' already exists\n",
    "```\n",
    "Solution: Use the cleanup cell below to drop the connection, then re-create it.\n",
    "\n",
    "**3. HTTP request timeout**\n",
    "```\n",
    "Connection timed out\n",
    "```\n",
    "Solution: Verify the AgentCore MCP server is running. Check with `cd neo4j-agentcore-mcp-server && ./cloud.sh`.\n",
    "\n",
    "**4. Authentication failed**\n",
    "```\n",
    "401 Unauthorized\n",
    "```\n",
    "Solution: The Cognito credentials may be invalid. Re-run:\n",
    "```\n",
    "cd neo4j-agentcore-mcp-server && ./deploy.sh credentials\n",
    "cd ../databrick_samples && ./setup_databricks_secrets.sh\n",
    "```\n",
    "\n",
    "**5. Tool not found**\n",
    "```\n",
    "Unknown tool: get-schema\n",
    "```\n",
    "Solution: Use the Gateway-prefixed tool name: `neo4j-mcp-server-target___get-schema`"
   ],
   "id": "markdown-d1eb7455"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Use this cell to drop the HTTP connection if you need to recreate it or clean up resources."
   ],
   "id": "markdown-abd94f17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below to drop the connection\n",
    "# spark.sql(f\"DROP CONNECTION IF EXISTS {CONNECTION_NAME}\")\n",
    "# print(f\"Connection '{CONNECTION_NAME}' dropped.\")"
   ],
   "id": "code-ba17d3ea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a working HTTP connection to Neo4j via AWS AgentCore:\n",
    "\n",
    "1. **Explore the schema**: Use `get-schema` to understand your graph structure\n",
    "2. **Write analytics queries**: Use `read-cypher` to query graph data for your analytics\n",
    "3. **Join with Delta tables**: Combine graph query results with your Delta Lake data\n",
    "4. **Share the connection**: Grant `USE CONNECTION` to other users via Unity Catalog\n",
    "5. **Deploy the agent**: Run `neo4j-mcp-agent-deploy.ipynb` to deploy the LangGraph agent\n",
    "\n",
    "For more information:\n",
    "- [Databricks HTTP Connections](https://docs.databricks.com/aws/en/query-federation/http)\n",
    "- [Databricks External MCP Servers](https://docs.databricks.com/aws/en/generative-ai/mcp/external-mcp)\n",
    "- [Neo4j Cypher Query Language](https://neo4j.com/docs/cypher-manual/current/)\n",
    "- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)"
   ],
   "id": "markdown-cdd48478"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}