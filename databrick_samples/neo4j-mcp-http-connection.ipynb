{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j MCP Server HTTP Connection Setup (AWS AgentCore)\n",
    "\n",
    "This notebook demonstrates how to create a Databricks HTTP connection to the Neo4j MCP server deployed on AWS AgentCore Gateway. Once configured, you can query Neo4j graph data directly from SQL using the `http_request` function.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Validates that OAuth2 secrets are configured in Databricks\n",
    "2. Creates an HTTP connection in Unity Catalog with OAuth2 M2M authentication\n",
    "3. Tests the connection by calling MCP tools (get-schema, read-cypher)\n",
    "4. Demonstrates how to parse and use the results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Databricks Runtime**: 15.4 LTS or later, or SQL warehouse 2023.40+\n",
    "- **Unity Catalog**: Must be enabled on your workspace\n",
    "- **AWS AgentCore deployed**: The Neo4j MCP server must be deployed via `neo4j-agentcore-mcp-server`\n",
    "- **Secrets configured**: Run `setup_databricks_secrets.sh` before this notebook\n",
    "\n",
    "## Authentication\n",
    "\n",
    "This integration uses **OAuth2 Machine-to-Machine (M2M)** authentication via AWS Cognito. Databricks handles token exchange and refresh automatically - no manual token management is required.\n",
    "\n",
    "## Security Note\n",
    "\n",
    "This integration provides **READ-ONLY** access to Neo4j. The `write-cypher` tool is intentionally excluded to prevent accidental data modifications from analytics workflows."
   ],
   "id": "markdown-3d3e6b1d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these values to match your environment. The secret scope should match what you used when running `setup_databricks_secrets.sh`."
   ],
   "id": "markdown-b4853132"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - update these values for your environment\n",
    "SECRET_SCOPE = \"mcp-neo4j-secrets\"           # Must match the scope used in setup_databricks_secrets.sh\n",
    "CONNECTION_NAME = \"neo4j_agentcore_mcp\"      # Name for the HTTP connection (metastore-level, not catalog-scoped)\n",
    "\n",
    "# Tool names are prefixed by AgentCore Gateway with the target name\n",
    "TOOL_GET_SCHEMA = \"neo4j-mcp-server-target___get-schema\"\n",
    "TOOL_READ_CYPHER = \"neo4j-mcp-server-target___read-cypher\""
   ],
   "id": "code-aa831383"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Validate Secrets\n",
    "\n",
    "First, verify that the required OAuth2 secrets exist in Databricks. If this step fails, run `setup_databricks_secrets.sh` from your local machine."
   ],
   "id": "markdown-8597fb38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that OAuth2 secrets are configured\n",
    "try:\n",
    "    gateway_host = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"gateway_host\")\n",
    "    client_id = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"client_id\")\n",
    "    client_secret = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"client_secret\")\n",
    "    token_endpoint = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"token_endpoint\")\n",
    "    oauth_scope = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"oauth_scope\")\n",
    "    \n",
    "    print(f\"OAuth2 secrets validated successfully!\")\n",
    "    print(f\"  Gateway Host: {gateway_host}\")\n",
    "    print(f\"  Client ID: {client_id}\")\n",
    "    print(f\"  Client Secret: [REDACTED - {len(client_secret)} characters]\")\n",
    "    print(f\"  Token Endpoint: {token_endpoint}\")\n",
    "    print(f\"  OAuth Scope: {oauth_scope}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to retrieve secrets from scope '{SECRET_SCOPE}'\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\")\n",
    "    print(\"To fix this, run the setup script from your local machine:\")\n",
    "    print(f\"  cd databrick_samples && ./setup_databricks_secrets.sh {SECRET_SCOPE}\")\n",
    "    print(\"\")\n",
    "    print(\"Make sure the AgentCore MCP server is deployed first:\")\n",
    "    print(\"  cd neo4j-agentcore-mcp-server && ./deploy.sh && ./deploy.sh credentials\")\n",
    "    raise"
   ],
   "id": "code-4124111c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the HTTP Connection\n",
    "\n",
    "Create an HTTP connection in Unity Catalog. This connection:\n",
    "- Points to the AgentCore Gateway endpoint\n",
    "- Uses OAuth2 M2M authentication with Cognito credentials\n",
    "- Databricks automatically handles token exchange and refresh\n",
    "- Can be shared with other users via Unity Catalog permissions\n",
    "\n",
    "**Implementation Note**: We use `spark.sql()` with Python f-strings instead of `%%sql` magic because the `CREATE CONNECTION` OPTIONS clause requires constant expressions.\n",
    "\n",
    "**Scope Note**: Connections are **metastore-level objects** in Unity Catalog, not catalog-scoped. They're shared across all catalogs in your workspace.\n",
    "\n",
    "**MCP Integration Note**: For full Databricks AI/MCP integration features, you may need to manually enable the \"Is mcp connection\" checkbox in the Catalog Explorer UI after creating the connection. Navigate to: **Catalog > External Data > Connections > neo4j_agentcore_mcp > Edit** and check the MCP option.\n",
    "\n",
    "**Note**: If the connection already exists, you'll see an error. Use the cleanup cell at the bottom to drop it first, then re-run this cell."
   ],
   "id": "markdown-f8fbde97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display connection parameters (secrets are not exposed)\n",
    "print(f\"Creating HTTP connection '{CONNECTION_NAME}'...\")\n",
    "print(f\"  Gateway Host: {gateway_host}\")\n",
    "print(f\"  Base Path: /mcp\")\n",
    "print(f\"  Authentication: OAuth2 M2M (Databricks handles token refresh)\")\n",
    "print(f\"  Secret Scope: {SECRET_SCOPE}\")"
   ],
   "id": "code-e23b874c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HTTP connection with OAuth2 M2M authentication\n",
    "# Databricks automatically handles token exchange and refresh\n",
    "# Note: Using spark.sql() because OPTIONS requires constant expressions (no variable interpolation in %%sql)\n",
    "# Note: Connections are metastore-level objects, not catalog-scoped\n",
    "\n",
    "create_connection_sql = f\"\"\"\n",
    "CREATE CONNECTION IF NOT EXISTS {CONNECTION_NAME} TYPE HTTP\n",
    "OPTIONS (\n",
    "  host secret('{SECRET_SCOPE}', 'gateway_host'),\n",
    "  base_path '/mcp',\n",
    "  client_id secret('{SECRET_SCOPE}', 'client_id'),\n",
    "  client_secret secret('{SECRET_SCOPE}', 'client_secret'),\n",
    "  oauth_scope secret('{SECRET_SCOPE}', 'oauth_scope'),\n",
    "  token_endpoint secret('{SECRET_SCOPE}', 'token_endpoint')\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing SQL:\")\n",
    "print(create_connection_sql)\n",
    "\n",
    "spark.sql(create_connection_sql)\n",
    "print(f\"\\nConnection created: {CONNECTION_NAME}\")"
   ],
   "id": "code-f70f08bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection was created\n",
    "print(f\"Connection '{CONNECTION_NAME}' created successfully!\")\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"IMPORTANT: Manual Step Required for MCP Integration\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\")\n",
    "print(\"The connection is created but won't appear in the Playground\")\n",
    "print(\"'External MCP Servers' dropdown until you enable the MCP flag:\")\n",
    "print(\"\")\n",
    "print(\"  1. Go to Catalog Explorer > External Data > Connections\")\n",
    "print(f\"  2. Find '{CONNECTION_NAME}' and click Edit\")\n",
    "print(\"  3. Check the 'Is MCP connection' checkbox\")\n",
    "print(\"  4. Save the connection\")\n",
    "print(\"\")\n",
    "print(\"After that, you can use it in the Playground:\")\n",
    "print(\"  Add tools > MCP Servers > External MCP Servers > Unity Catalog Connection\")\n",
    "print(\"\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "code-9d5a7624"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Connection - List Tools\n",
    "\n",
    "The MCP protocol uses JSON-RPC. Let's first list the available tools to verify the connection works.\n",
    "\n",
    "**Note**: Tool names are prefixed by the AgentCore Gateway with the target name:\n",
    "- `neo4j-mcp-server-target___get-schema`\n",
    "- `neo4j-mcp-server-target___read-cypher`"
   ],
   "id": "markdown-fe78693f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# MCP JSON-RPC request to list tools\n",
    "list_tools_request = json.dumps({\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/list\",\n",
    "    \"id\": 1\n",
    "})\n",
    "\n",
    "print(\"Request payload:\")\n",
    "print(list_tools_request)"
   ],
   "id": "code-fbe42813"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available MCP tools\n",
    "list_tools_sql = f\"\"\"\n",
    "SELECT http_request(\n",
    "  conn => '{CONNECTION_NAME}',\n",
    "  method => 'POST',\n",
    "  path => '',\n",
    "  headers => map('Content-Type', 'application/json'),\n",
    "  json => '{{\"jsonrpc\":\"2.0\",\"method\":\"tools/list\",\"id\":1}}'\n",
    ") AS response\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(list_tools_sql)\n",
    "display(result)"
   ],
   "id": "code-c0e3af56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get Neo4j Schema\n",
    "\n",
    "Call the `get-schema` tool to retrieve the Neo4j database schema, including node labels, relationship types, and properties.\n",
    "\n",
    "**Note**: The tool name is prefixed: `neo4j-mcp-server-target___get-schema`"
   ],
   "id": "markdown-058ef9c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Neo4j database schema using the Gateway-prefixed tool name\n",
    "get_schema_sql = f\"\"\"\n",
    "SELECT http_request(\n",
    "  conn => '{CONNECTION_NAME}',\n",
    "  method => 'POST',\n",
    "  path => '',\n",
    "  headers => map('Content-Type', 'application/json'),\n",
    "  json => '{{\"jsonrpc\":\"2.0\",\"method\":\"tools/call\",\"params\":{{\"name\":\"{TOOL_GET_SCHEMA}\",\"arguments\":{{}}}},\"id\":2}}'\n",
    ") AS response\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(get_schema_sql)\n",
    "display(result)"
   ],
   "id": "code-7387d32a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execute a Read Query\n",
    "\n",
    "Call the `read-cypher` tool to execute a read-only Cypher query against Neo4j.\n",
    "\n",
    "**Important**: Only read queries are permitted through this connection. The `write-cypher` tool is intentionally not exposed.\n",
    "\n",
    "**Note**: The tool name is prefixed: `neo4j-mcp-server-target___read-cypher`"
   ],
   "id": "markdown-897f14a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example: Count nodes by label\n",
    "cypher_query = \"MATCH (n) RETURN labels(n) AS label, count(*) AS count ORDER BY count DESC LIMIT 10\"\n",
    "\n",
    "# Build the MCP request with Gateway-prefixed tool name\n",
    "read_cypher_request = json.dumps({\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"tools/call\",\n",
    "    \"params\": {\n",
    "        \"name\": TOOL_READ_CYPHER,\n",
    "        \"arguments\": {\n",
    "            \"query\": cypher_query\n",
    "        }\n",
    "    },\n",
    "    \"id\": 3\n",
    "})\n",
    "\n",
    "print(\"Cypher query:\")\n",
    "print(cypher_query)\n",
    "print(\"\")\n",
    "print(\"MCP request:\")\n",
    "print(read_cypher_request)"
   ],
   "id": "code-e813748d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a read-only Cypher query\n",
    "# Escape single quotes for SQL\n",
    "escaped_request = read_cypher_request.replace(\"'\", \"''\")\n",
    "\n",
    "cypher_sql = f\"\"\"\n",
    "SELECT http_request(\n",
    "  conn => '{CONNECTION_NAME}',\n",
    "  method => 'POST',\n",
    "  path => '',\n",
    "  headers => map('Content-Type', 'application/json'),\n",
    "  json => '{escaped_request}'\n",
    ") AS response\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(cypher_sql)\n",
    "display(result)"
   ],
   "id": "code-18c3908b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Parse and Display Results\n",
    "\n",
    "The MCP response is JSON. Let's parse it and display the results in a more readable format."
   ],
   "id": "markdown-729efd61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, get_json_object\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
    "\n",
    "# Execute the query and get the response\n",
    "escaped_request = read_cypher_request.replace(\"'\", \"''\")\n",
    "\n",
    "result_df = spark.sql(f\"\"\"\n",
    "    SELECT http_request(\n",
    "      conn => '{CONNECTION_NAME}',\n",
    "      method => 'POST',\n",
    "      path => '',\n",
    "      headers => map('Content-Type', 'application/json'),\n",
    "      json => '{escaped_request}'\n",
    "    ) AS response\n",
    "\"\"\")\n",
    "\n",
    "# Extract the response body\n",
    "response_row = result_df.first()\n",
    "if response_row:\n",
    "    response = response_row[\"response\"]\n",
    "    print(\"Raw response:\")\n",
    "    print(json.dumps(json.loads(response[\"text\"]), indent=2) if \"text\" in response else response)"
   ],
   "id": "code-3e5de755"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Query Neo4j\n",
    "\n",
    "Here's a reusable function to query Neo4j through the MCP connection."
   ],
   "id": "markdown-79f349f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def query_neo4j(cypher_query: str, connection_name: str = None, tool_name: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Execute a read-only Cypher query against Neo4j via the MCP HTTP connection.\n",
    "    \n",
    "    Args:\n",
    "        cypher_query: The Cypher query to execute (read-only)\n",
    "        connection_name: Name of the HTTP connection (default: uses CONNECTION_NAME from notebook)\n",
    "        tool_name: The MCP tool name (default: uses TOOL_READ_CYPHER with Gateway prefix)\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON response from Neo4j\n",
    "    \"\"\"\n",
    "    if connection_name is None:\n",
    "        connection_name = CONNECTION_NAME\n",
    "    if tool_name is None:\n",
    "        tool_name = TOOL_READ_CYPHER\n",
    "    \n",
    "    # Build the MCP request with Gateway-prefixed tool name\n",
    "    request_payload = json.dumps({\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"tools/call\",\n",
    "        \"params\": {\n",
    "            \"name\": tool_name,\n",
    "            \"arguments\": {\n",
    "                \"query\": cypher_query\n",
    "            }\n",
    "        },\n",
    "        \"id\": 1\n",
    "    })\n",
    "    \n",
    "    # Escape single quotes for SQL\n",
    "    escaped_payload = request_payload.replace(\"'\", \"''\")\n",
    "    \n",
    "    # Execute the query\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "        SELECT http_request(\n",
    "          conn => '{connection_name}',\n",
    "          method => 'POST',\n",
    "          path => '',\n",
    "          headers => map('Content-Type', 'application/json'),\n",
    "          json => '{escaped_payload}'\n",
    "        ) AS response\n",
    "    \"\"\")\n",
    "    \n",
    "    # Parse the response\n",
    "    response_row = result_df.first()\n",
    "    if response_row and \"response\" in response_row.asDict():\n",
    "        response = response_row[\"response\"]\n",
    "        if \"text\" in response:\n",
    "            return json.loads(response[\"text\"])\n",
    "    return None\n",
    "\n",
    "print(\"Helper function 'query_neo4j' is now available.\")\n",
    "print(\"\")\n",
    "print(\"Example usage:\")\n",
    "print(f'  result = query_neo4j(\"MATCH (n) RETURN count(n) AS total\")')\n",
    "print(f\"  # Uses connection: {CONNECTION_NAME}\")\n",
    "print(f\"  # Uses tool: {TOOL_READ_CYPHER}\")"
   ],
   "id": "code-cffd229e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use the helper function\n",
    "result = query_neo4j(\"MATCH (n) RETURN count(n) AS total\")\n",
    "print(\"Query result:\")\n",
    "print(json.dumps(result, indent=2))"
   ],
   "id": "code-1697641a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**1. Secret not found**\n",
    "```\n",
    "Secret does not exist with scope: mcp-neo4j-secrets and key: gateway_host\n",
    "```\n",
    "Solution: Run `./setup_databricks_secrets.sh` from the `databrick_samples` directory.\n",
    "\n",
    "**2. Connection already exists**\n",
    "```\n",
    "Connection 'neo4j_agentcore_mcp' already exists\n",
    "```\n",
    "Solution: Use the cleanup cell below to drop the connection, then re-create it.\n",
    "\n",
    "**3. HTTP request timeout**\n",
    "```\n",
    "Connection timed out\n",
    "```\n",
    "Solution: Verify the AgentCore MCP server is running. Check with `cd neo4j-agentcore-mcp-server && ./cloud.sh`.\n",
    "\n",
    "**4. Authentication failed**\n",
    "```\n",
    "401 Unauthorized\n",
    "```\n",
    "Solution: The Cognito credentials may be invalid. Re-run:\n",
    "```\n",
    "cd neo4j-agentcore-mcp-server && ./deploy.sh credentials\n",
    "cd ../databrick_samples && ./setup_databricks_secrets.sh\n",
    "```\n",
    "\n",
    "**5. Tool not found**\n",
    "```\n",
    "Unknown tool: get-schema\n",
    "```\n",
    "Solution: Use the Gateway-prefixed tool name: `neo4j-mcp-server-target___get-schema`"
   ],
   "id": "markdown-d1eb7455"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Use this cell to drop the HTTP connection if you need to recreate it or clean up resources."
   ],
   "id": "markdown-abd94f17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below to drop the connection\n",
    "# spark.sql(f\"DROP CONNECTION IF EXISTS {CONNECTION_NAME}\")\n",
    "# print(f\"Connection '{CONNECTION_NAME}' dropped.\")"
   ],
   "id": "code-ba17d3ea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a working HTTP connection to Neo4j via AWS AgentCore:\n",
    "\n",
    "1. **Explore the schema**: Use `get-schema` to understand your graph structure\n",
    "2. **Write analytics queries**: Use `read-cypher` to query graph data for your analytics\n",
    "3. **Join with Delta tables**: Combine graph query results with your Delta Lake data\n",
    "4. **Share the connection**: Grant `USE CONNECTION` to other users via Unity Catalog\n",
    "5. **Deploy the agent**: Run `neo4j-mcp-agent-deploy.ipynb` to deploy the LangGraph agent\n",
    "\n",
    "For more information:\n",
    "- [Databricks HTTP Connections](https://docs.databricks.com/aws/en/query-federation/http)\n",
    "- [Databricks External MCP Servers](https://docs.databricks.com/aws/en/generative-ai/mcp/external-mcp)\n",
    "- [Neo4j Cypher Query Language](https://neo4j.com/docs/cypher-manual/current/)\n",
    "- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)"
   ],
   "id": "markdown-cdd48478"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}